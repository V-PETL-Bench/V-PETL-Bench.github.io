<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="V-PETL Bench: A Unified Visual Parameter-Efficient Transfer Learning Benchmark">
    <meta name="author"
          content="Yi Xin, Siqi Luo, Xuyang Liu, Haodi Zhou, Xinyu Cheng, Christina Lee, Junlong Du, Yuntao Du, Haozhe Wang, Mingcai Chen, Ting Liu, Guimin Hu, Zhongwei Wan, Rongchao Zhang, Aoxue Li, Mingyang Yi, Xiaohong Liu">

    <title>V-PETL Bench: A Unified Visual Parameter-Efficient Transfer Learning Benchmark</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>
<body>
    <div class="jumbotron jumbotron-fluid">
        <div class="container"></div>
        <h2>V-PETL Bench: A Unified Visual Parameter-Efficient Transfer Learning Benchmark</h2>
        <hr>
        <p class="authors">
            <a href="https://synbol.github.io/">Yi Xin<sup>1</sup></a>,
            <a>Siqi Luo<sup>1,2</sup></a>,
            <a>Xuyang Liu<sup>3</sup></a>,
            <a>Xuyang Liu<sup>3</sup></a>,
            <a>Haodi Zhou<sup>1</sup></a>,
            <a>Xinyu Cheng<sup>1</sup></a>,
            <a>Christina Lee<sup>4</sup></a>,
            <a>Junlong Du<sup>5</sup></a>,
            <a>Yuntao Du<sup>6</sup></a>,
            <a>Haozhe Wang<sup>7</sup></a>,
            
        </p>

        <p class="authors">
            <a>Mingcai Chen<sup>1</sup></a>,
            <a>Ting Liu<sup>8</sup></a>,
            <a>Guimin Hu<sup>9</sup></a>,
            <a>Zhongwei Wan<sup>9</sup></a>,
            <a>Rongchao Zhang<sup>9</sup></a>,
            <a>Aoxue Li<sup>9</sup></a>,
            <a>Mingyang Yi<sup>9</sup></a>,
            <a>Xiaohong Liu<sup>9</sup></a>,
        </p>  
        <p class="authors">
            <sup>1</sup>Nanjing University
            <sup>2</sup>Shanghai Jiaotong University
            <sup>3</sup>Sichuan University
            <sup>4</sup>Massachusetts Institute of Technology
            <sup>5</sup>Youtu Lab, Tencent
            <sup>6</sup>BIGAI
            
        </p>

        <p class="authors">
            <sup>7</sup>Hong Kong University of Science and Technology
            <sup>8</sup>NUDT
            <sup>9</sup>University of Copenhagen
            <sup>10</sup>Ohio State University
            <sup>11</sup>Peking University
            <sup>11</sup>Huawei Noahâ€™s Ark Lab
        </p>   
        <p>
            <i>Feel to contact me if you have questions!</i>
        </p>
        <div class="btn-group" role="group" aria-label="Top menu">
            <a class="btn btn-primary" href="">Paper</a>
            <a class="btn btn-primary" href="">Supplementary</a>
            <a class="btn btn-primary" href="https://github.com/synbol/Parameter-Efficient-Transfer-Learning-Benchmark">Benchmark Code</a>
            <a class="btn btn-primary" href="https://github.com/synbol/Parameter-Efficient-Transfer-Learning-Benchmark">Dataset</a>
        </div>
        
    </div>
    <div class="container">
        
    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
            Parameter-efficient transfer learning (PETL) methods show promise in adapting a pre-trained model to various downstream tasks while training only a few parameters. 
            In the computer vision (CV) domain, numerous PETL algorithms have been proposed, but their direct employment or comparison remains inconvenient.
            To address this challenge, we construct a Unified Visual PETL Benchmark (V-PETL Bench) for the CV domain by selecting <b> 30 diverse, challenging, and comprehensive datasets </b> from image recognition, video action recognition, and dense prediction tasks.
            On these datasets, we systematically evaluate <b> 25 dominant PETL algorithms </b> and open-source a modular and extensible codebase for fair evaluation of these algorithms. V-PETL Bench runs on NVIDIA A800 GPUs and requires approximately <b> 310 GPU days </b>. 
            We release all the checkpoints and training logs, making it more efficient and friendly to researchers. Additionally, V-PETL Bench will be continuously updated for new PETL algorithms and CV tasks.
        </p>
    </div>

    <div class="section">
        <h2>
            PETL Algorithms
        </h2>
        <hr>
        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="" style="width:100%">
            </div>
        </div>
        <p>
            In visual PETL survey, existing PETL methods can be divided into 7 basic categories. For each category, we select multiple algorithms for implementation within the V-PETL Bench.
            We implement <b>2 traditional and 25 PETL algorithms</b> in the codebase for V-PETL Bench,including Full fine-tuning, Frozen, Adapter, AdaptFormer, SCT, BitFit, U-Tuning, VPT-shallow,
            VPT-Deep, Prefix Tuning, SSF, LoRA, NOAHFacT, RepAdapter, Hydra, LST, DTL, HST, GPS, LAST, SNF, BAPAT, LNTUNE, LoRand, E3VA, and Mona. 
            The algorithms are chosen based on the following considerations: 
            1) The algorithm is commonly used in the visual PETL domain and has considerable influence;
            2) The algorithm corresponds with the comprehensive timeline of visual PETL development.
        </p>
    </div>


    <div class="section">
        <h2>
            Tasks and Datasets
        </h2>
        <hr>
        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="" style="width:100%">
            </div>
        </div>
        <p>
            The V-PETL Bench includes <b>30 datasets from image recognition, video action recognition, and dense prediction tasks</b>. 
            Each dataset in the V-PETL Bench is under a permissive license that allows usage for research purposes. 
            These datasets are chosen based on the following considerations: 
            1) The dataset represents a mainstream CV task and is broadly relevant to PETL; 
            2) The dataset is diverse and covers multiple domains; 
            3) The training process is environmentally sustainable and affordable for research labs in both industry and academia.
        </p>
    </div>


    <div class="section">
        <h2>
             Performance-Parameter Trade-off Metric
        </h2>
        <hr>
        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="" style="width:100%">
            </div>
        </div>
        <p>
            For the evaluation of PETL algorithms, to compare different methods with a single number that considers both task performance and parameter-efficiency, 
            we define the Performance-Parameter Trade-off (PPT) metric. C is a normalization constant set at 10^7, aligns with the typical parameter sizes of existing PETL algorithms. 
        </p>
    </div>

    <div class="section">
        <h2>
             Benchmark and Evaluations
        </h2>
        <hr>
        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="" style="width:100%">
            </div>
        </div>
        <p>
            
        </p>
    </div>


    <div class="section">
        <h2>
             Visualization
        </h2>
        <hr>
        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="" style="width:100%">
            </div>
        </div>
        <p>
            
        </p>
    </div>



    <div class="section">
        <h2>
             Codebase Structure
        </h2>
        <hr>
        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="" style="width:100%">
            </div>
        </div>
        <p>
            We provide an overview of the codebase structure of V-PETL Bench, which which is organized into four abstract layers.
            In the core layer, we implement the essential functions commonly used for training PETL algorithms.
            In the algorithm layer, we first implement the base class for PETL algorithms, which includes initializing the datasets, data loaders, and models from the core layer.
            The extension layer is dedicated to advancing the core PETL algorithms for visual analysis.
            We encapsulate the core functions and algorithms within the API layer, creating a user-friendly interface for individuals from diverse backgrounds who are interested in applying PETL algorithms to new applications.
        </p>
    </div>





<!-- 
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
crossorigin="anonymous"></script> -->
</body>
</html>
