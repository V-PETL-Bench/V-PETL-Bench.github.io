<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="V-PETL Bench: A Unified Visual Parameter-Efficient Transfer Learning Benchmark">
    <meta name="author"
          content="Yi Xin, Siqi Luo, Xuyang Liu, Haodi Zhou, Xinyu Cheng, Christina Lee, Junlong Du, Yuntao Du, Haozhe Wang, Mingcai Chen, Ting Liu, Guimin Hu, Zhongwei Wan, Rongchao Zhang, Aoxue Li, Mingyang Yi, Xiaohong Liu">

    <title>V-PETL Bench: A Unified Visual Parameter-Efficient Transfer Learning Benchmark</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>
<body>
    <div class="jumbotron jumbotron-fluid">
        <div class="container"></div>
        <h2>V-PETL Bench: A Unified Visual Parameter-Efficient Transfer Learning Benchmark</h2>
        <hr>
        <p class="authors">
            <a href="https://synbol.github.io/">Yi Xin<sup>1</sup></a>,
            <a>Siqi Luo<sup>1,2</sup></a>,
            <a>Xuyang Liu<sup>3</sup></a>,
            <a>Xuyang Liu<sup>3</sup></a>,
            <a>Haodi Zhou<sup>1</sup></a>,
            <a>Xinyu Cheng<sup>1</sup></a>,
            <a>Christina Lee<sup>4</sup></a>,
            <a>Junlong Du<sup>5</sup></a>,
            <a>Yuntao Du<sup>6</sup></a>,
            <a>Haozhe Wang<sup>7</sup></a>,
            <a>Mingcai Chen<sup>1</sup></a>,
            <a>Ting Liu<sup>8</sup></a>,
            <a>Guimin Hu<sup>9</sup></a>,
            <a>Zhongwei Wan<sup>9</sup></a>,
        </p>
        <p class="authors">
            <sup>1</sup>Nanjing University
            <sup>2</sup>Shanghai Jiaotong University
            <sup>3</sup>Sichuan University
            <sup>4</sup>MIT
            <sup>5</sup>Tencent
            <sup>6</sup>BIGAI
            <sup>7</sup>HKUST
            <sup>8</sup>NUDT
            <sup>9</sup>
            <sup>10</sup>
            
            
        </p>
<!--         <p>
            <i>Accepted at NeurIPS 2023 Datasets and Benchmark Track!</i>
        </p> -->
        <div class="btn-group" role="group" aria-label="Top menu">
            <a class="btn btn-primary" href="">Paper</a>
            <a class="btn btn-primary" href="">Supplementary</a>
            <a class="btn btn-primary" href="">Benchmark Code</a>
            <a class="btn btn-primary" href="">Benchmark Dataset</a>
        </div>
        
    </div>
    <div class="container">
        
    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
            Parameter-efficient transfer learning (PETL) methods show promise in adapting a pre-trained model to various downstream tasks while training only a few parameters. 
            In the computer vision (CV) domain, numerous PETL algorithms have been proposed, but their direct employment or comparison remains inconvenient.
            To address this challenge, we construct a Unified Visual PETL Benchmark (V-PETL Bench) for the CV domain by selecting <b> 30 diverse, challenging, and comprehensive datasets </b> from image recognition, video action recognition, and dense prediction tasks.
            On these datasets, we systematically evaluate <b> 25 dominant PETL algorithms </b> and open-source a modular and extensible codebase for fair evaluation of these algorithms. V-PETL Bench runs on NVIDIA A800 GPUs and requires approximately <b> 310 GPU days </b>. 
            We release all the checkpoints and training logs, making it more efficient and friendly to researchers. Additionally, V-PETL Bench will be continuously updated for new PETL algorithms and CV tasks.
        </p>
    </div>


<!--     <div class="section">
        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="figures/corr_example.png" style="width:100%">
            </div>
        </div>
        <p>
            Multimodal adaptation methods are sensitive to image and text corruptions. The two rows show image captioning and visual question answering predicted by Adapter respectively. Blue boxes contain the original image and query text. Orange boxes present the corrupted images, texts and model output.
        </p>
    </div> -->

<!--     <div class="section">
        <h2>
            Model Adaptation Methods
        </h2>
        <hr>
        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="figures/adapt.png" style="width:100%">
            </div>
        </div>
        <p>
            We investigate the robustness of four mainstream adaptation methods: full fine-tuning, soft prompt, LoRA, and adapter-based methods including Adapter , Hyperformer, and Compacter. To better understand the robustness of these adaptation methods, we also consider the information sharing across tasks. Therefore, for soft prompt, LoRA, and Compacters, we conduct experiments in both single and multiple manners. The single manner uses one adaptation model for all tasks, while the multiple manner uses independent adaptation modules for different tasks. For Adapter, besides the single and multiple manners, we also adopt the half-shared manner, where only the undersampling module in adapters is shared across tasks. In total, we have eleven adaptation methods
        </p>
    </div> -->

<!--     <div class="section">
        <h2>
            Benchmark and Evaluations
        </h2>
        <hr>
        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="figures/corr-eg.png" style="width:100%">
            </div>
        </div>
        <p>
            
        </p>
        <div>

        </div>

        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="figures/corr-img.png" style="width:100%">
            </div>
        </div>

        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <!-- <img src="figures/corr_methods.png" style="width:100%"> -->
<!--                 <img src="figures/corr_text.png" style="width:80%">
            </div>
        </div>
        <p>
            We introduce 20 corruptions to image data. Except for the blank corruption, each type of corruption has five levels of severity. In total, there are 96 different corruptions.

            We have adopted a total of 35 corruption methods  which can be grouped into three categories: <i>character-level, word-level</i>, and <i>sentence-level</i> based on the level of corruption.
            We have also introduced various severity levels for text corruptions, as we have done for image corruptions. For character-level corruptions and some word-level corruptions, we apply five severity levels. However, for sentence-level corruptions and some word-level corruptions, only one perturbation is available. In total, we have 35 corruption methods along with 87 different perturbations.

        </p>

        <div class="section">
            <h2>
                Experimental Settings
            </h2>
            <hr>
            <div class="col-sm-12">
                <img src="figures/stat.png" style="width:100%">
            </div>
            <div class="col-sm-12">
                <img src="figures/ada-method.png" style="width:60%" class="center">
            </div>

            <p>
                Accuracy on the Karpathy-test split is evaluated for VQAv2. For GQA, accuracy on the test-dev split is evaluated, and accuracy on the test-P split is used for NLVR$^2$. In image captioning, we use CIDEr on the Karpathy-test split.
            </p>

        </div>   --> -->


    </div>

    <br>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            
        </div>
    </div>


        <footer>
            <p>
                Acknowledgement: This page is modified from <a href="https://yilundu.github.io/">Yilun Du</a> and <a href="https://www.cs.cmu.edu/~jielinq/">Jielin Qiu</a>.
            </p>
        </footer>
    </div>


<!-- 
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
crossorigin="anonymous"></script> -->
</body>
</html>
